{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "89859690",
   "metadata": {},
   "outputs": [],
   "source": "# Synthetic Time Series Data Generation\n\nThis notebook generates 10 different synthetic time series datasets for comparing traditional prediction intervals with conformal prediction methods.\n\nEach dataset is designed to test different assumptions and weaknesses of traditional prediction interval methods."
  },
  {
   "cell_type": "code",
   "id": "fdhsvbet7yg",
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nimport os\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create output directory\nos.makedirs('../data', exist_ok=True)\n\nprint(\"Setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ap07i4vmrb",
   "source": "## Dataset 1: Baseline AR(1) with Gaussian Noise\n\n**Purpose**: Ideal scenario where traditional methods should work well  \n**Model**: y_t = 0.7 * y_{t-1} + ε, where ε ~ N(0, 1)  \n**What's varying**: Nothing (baseline)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ysqspxqb3pb",
   "source": "# Dataset 1: Baseline AR(1) with Gaussian noise\nnp.random.seed(1)\nn = 2500\nar_coef = 0.7\n\n# Generate AR(1) process\ny = np.zeros(n)\ny[0] = np.random.normal(0, 1)\nfor t in range(1, n):\n    y[t] = ar_coef * y[t-1] + np.random.normal(0, 1)\n\n# Create DataFrame\ndf1 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'true_variance': 1.0  # constant variance\n})\n\n# Save to CSV\ndf1.to_csv('../data/dataset_1_baseline_ar1.csv', index=False)\nprint(f\"Dataset 1 generated: {len(df1)} observations\")\nprint(f\"Mean: {df1['value'].mean():.3f}, Std: {df1['value'].std():.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "mavd1rezgd",
   "source": "# Plot Dataset 1\nplt.figure(figsize=(12, 4))\nplt.plot(df1['timestamp'], df1['value'], linewidth=0.8)\nplt.title('Dataset 1: Baseline AR(1) with Gaussian Noise')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x0kzqte89bm",
   "source": "## Dataset 2: AR(1) with Heavy-Tailed Noise (Student-t)\n\n**Purpose**: Test robustness to outliers and fat tails  \n**Model**: y_t = 0.7 * y_{t-1} + ε, where ε ~ t(df=3)  \n**What's varying**: Noise distribution (violates Gaussian assumption)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gj17t7ha9wi",
   "source": "# Dataset 2: AR(1) with heavy-tailed noise (Student-t)\nnp.random.seed(2)\nn = 2500\nar_coef = 0.7\ndf_t = 3  # degrees of freedom for t-distribution\n\n# Generate AR(1) process with t-distributed noise\ny = np.zeros(n)\ny[0] = stats.t.rvs(df_t)\nfor t in range(1, n):\n    y[t] = ar_coef * y[t-1] + stats.t.rvs(df_t)\n\n# Create DataFrame\ndf2 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'noise_type': 'student_t',\n    'df': df_t\n})\n\n# Save to CSV\ndf2.to_csv('../data/dataset_2_heavy_tailed.csv', index=False)\nprint(f\"Dataset 2 generated: {len(df2)} observations\")\nprint(f\"Mean: {df2['value'].mean():.3f}, Std: {df2['value'].std():.3f}\")\nprint(f\"Kurtosis: {stats.kurtosis(df2['value']):.3f} (higher than Gaussian)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c7u5va59on",
   "source": "# Plot Dataset 2\nplt.figure(figsize=(12, 4))\nplt.plot(df2['timestamp'], df2['value'], linewidth=0.8, color='orange')\nplt.title('Dataset 2: AR(1) with Heavy-Tailed Noise (Student-t, df=3)')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hq8m18x4c8",
   "source": "## Dataset 3: AR(1) with Heteroscedastic Noise (GARCH-like)\n\n**Purpose**: Test whether intervals adapt to changing volatility  \n**Model**: y_t = 0.7 * y_{t-1} + σ_t * ε, where σ_t depends on recent squared residuals  \n**What's varying**: Variance changes over time (volatility clustering)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y9rwlc7do0p",
   "source": "# Dataset 3: AR(1) with GARCH-like heteroscedastic noise\nnp.random.seed(3)\nn = 2500\nar_coef = 0.7\n\n# GARCH(1,1) parameters\nalpha0 = 0.1\nalpha1 = 0.15  # ARCH effect\nbeta1 = 0.75   # GARCH effect\n\n# Generate AR(1) + GARCH process\ny = np.zeros(n)\nsigma2 = np.zeros(n)\nsigma2[0] = 1.0\ny[0] = np.random.normal(0, np.sqrt(sigma2[0]))\n\nfor t in range(1, n):\n    # Update conditional variance (GARCH equation)\n    sigma2[t] = alpha0 + alpha1 * (y[t-1] - ar_coef * y[t-2] if t > 1 else y[t-1])**2 + beta1 * sigma2[t-1]\n    # Generate observation\n    epsilon = np.random.normal(0, np.sqrt(sigma2[t]))\n    y[t] = ar_coef * y[t-1] + epsilon\n\n# Create DataFrame\ndf3 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'true_variance': sigma2\n})\n\n# Save to CSV\ndf3.to_csv('../data/dataset_3_garch.csv', index=False)\nprint(f\"Dataset 3 generated: {len(df3)} observations\")\nprint(f\"Mean: {df3['value'].mean():.3f}, Std: {df3['value'].std():.3f}\")\nprint(f\"Variance range: [{df3['true_variance'].min():.3f}, {df3['true_variance'].max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ugkt7mmww3d",
   "source": "# Plot Dataset 3\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n\nax1.plot(df3['timestamp'], df3['value'], linewidth=0.8, color='green')\nax1.set_title('Dataset 3: AR(1) with GARCH-like Heteroscedastic Noise')\nax1.set_ylabel('Value')\nax1.grid(True, alpha=0.3)\n\nax2.plot(df3['timestamp'], df3['true_variance'], linewidth=0.8, color='red')\nax2.set_ylabel('Conditional Variance')\nax2.set_xlabel('Time')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "92hmg5by0ne",
   "source": "## Dataset 4: AR(1) with Time-Varying Variance (Deterministic)\n\n**Purpose**: Test simpler heteroscedasticity with predictable pattern  \n**Model**: y_t = 0.7 * y_{t-1} + σ_t * ε, where σ_t = 1 + 2*sin(2πt/500)  \n**What's varying**: Variance follows a sinusoidal pattern",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ukc4vi0eiqs",
   "source": "# Dataset 4: AR(1) with time-varying variance (deterministic)\nnp.random.seed(4)\nn = 2500\nar_coef = 0.7\n\n# Time-varying standard deviation (sinusoidal pattern)\nt = np.arange(n)\nsigma_t = 1 + 2 * np.sin(2 * np.pi * t / 500)\n\n# Generate AR(1) process with time-varying variance\ny = np.zeros(n)\ny[0] = np.random.normal(0, sigma_t[0])\nfor i in range(1, n):\n    y[i] = ar_coef * y[i-1] + sigma_t[i] * np.random.normal(0, 1)\n\n# Create DataFrame\ndf4 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'true_variance': sigma_t**2\n})\n\n# Save to CSV\ndf4.to_csv('../data/dataset_4_timevarying_variance.csv', index=False)\nprint(f\"Dataset 4 generated: {len(df4)} observations\")\nprint(f\"Mean: {df4['value'].mean():.3f}, Std: {df4['value'].std():.3f}\")\nprint(f\"Variance range: [{df4['true_variance'].min():.3f}, {df4['true_variance'].max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "v9zzl2fjhl",
   "source": "# Plot Dataset 4\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n\nax1.plot(df4['timestamp'], df4['value'], linewidth=0.8, color='purple')\nax1.set_title('Dataset 4: AR(1) with Deterministic Time-Varying Variance')\nax1.set_ylabel('Value')\nax1.grid(True, alpha=0.3)\n\nax2.plot(df4['timestamp'], df4['true_variance'], linewidth=0.8, color='red')\nax2.set_ylabel('Variance (sinusoidal)')\nax2.set_xlabel('Time')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tu624cjt5u",
   "source": "## Dataset 5: Regime-Switching Model\n\n**Purpose**: Test adaptivity to distribution shifts  \n**Model**: AR(1) with two regimes - low volatility (σ=0.5) and high volatility (σ=2)  \n**What's varying**: Abrupt changes in volatility (concept drift)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kfmbaeoss8q",
   "source": "# Dataset 5: Regime-switching model\nnp.random.seed(5)\nn = 2500\nar_coef = 0.7\n\n# Define regimes\nlow_vol = 0.5\nhigh_vol = 2.0\n\n# Generate regime switches every 300-500 steps\nregime = np.zeros(n)\nregime_indicator = np.zeros(n)\ncurrent_pos = 0\ncurrent_regime = 0  # 0 = low vol, 1 = high vol\n\nwhile current_pos < n:\n    # Random duration between 300 and 500\n    duration = np.random.randint(300, 501)\n    end_pos = min(current_pos + duration, n)\n    \n    regime[current_pos:end_pos] = low_vol if current_regime == 0 else high_vol\n    regime_indicator[current_pos:end_pos] = current_regime\n    \n    current_pos = end_pos\n    current_regime = 1 - current_regime  # Switch regime\n\n# Generate AR(1) process with regime-switching volatility\ny = np.zeros(n)\ny[0] = np.random.normal(0, regime[0])\nfor t in range(1, n):\n    y[t] = ar_coef * y[t-1] + regime[t] * np.random.normal(0, 1)\n\n# Create DataFrame\ndf5 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'regime': regime_indicator.astype(int),\n    'true_variance': regime**2\n})\n\n# Save to CSV\ndf5.to_csv('../data/dataset_5_regime_switching.csv', index=False)\nprint(f\"Dataset 5 generated: {len(df5)} observations\")\nprint(f\"Mean: {df5['value'].mean():.3f}, Std: {df5['value'].std():.3f}\")\nprint(f\"Regime changes: {np.sum(np.diff(df5['regime']) != 0)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z5exesg9xep",
   "source": "# Plot Dataset 5\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n\nax1.plot(df5['timestamp'], df5['value'], linewidth=0.8, color='brown')\nax1.set_title('Dataset 5: Regime-Switching Model')\nax1.set_ylabel('Value')\nax1.grid(True, alpha=0.3)\n\nax2.fill_between(df5['timestamp'], 0, df5['regime'], alpha=0.5, color='gray', label='Regime')\nax2.set_ylabel('Regime (0=low, 1=high)')\nax2.set_xlabel('Time')\nax2.set_ylim([-0.1, 1.1])\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "svyn6gondi",
   "source": "## Dataset 6: Non-Stationary with Trend Breaks\n\n**Purpose**: Test how methods handle non-stationarity  \n**Model**: AR(1) with 2-3 structural breaks where intercept changes  \n**What's varying**: Mean shifts at random points",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q9y46iqqqbk",
   "source": "# Dataset 6: Non-stationary with trend breaks\nnp.random.seed(6)\nn = 2500\nar_coef = 0.7\n\n# Define structural break points and intercepts\nbreak_points = [0, 800, 1500, 2100, n]\nintercepts = [0, 3, -2, 1]\n\n# Generate AR(1) process with structural breaks\ny = np.zeros(n)\nbreak_indicator = np.zeros(n)\ncurrent_intercept = np.zeros(n)\n\nfor i in range(len(break_points) - 1):\n    start = break_points[i]\n    end = break_points[i + 1]\n    current_intercept[start:end] = intercepts[i]\n    break_indicator[start:end] = i\n\ny[0] = intercepts[0] + np.random.normal(0, 1)\nfor t in range(1, n):\n    # Find current intercept\n    segment = int(break_indicator[t])\n    intercept = intercepts[segment]\n    y[t] = intercept + ar_coef * (y[t-1] - intercepts[segment-1] if t > 0 and segment > 0 else y[t-1]) + np.random.normal(0, 1)\n\n# Create DataFrame\ndf6 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'intercept': current_intercept,\n    'break_segment': break_indicator.astype(int)\n})\n\n# Save to CSV\ndf6.to_csv('../data/dataset_6_trend_breaks.csv', index=False)\nprint(f\"Dataset 6 generated: {len(df6)} observations\")\nprint(f\"Mean: {df6['value'].mean():.3f}, Std: {df6['value'].std():.3f}\")\nprint(f\"Break points: {break_points[1:-1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f2b60an70h4",
   "source": "# Plot Dataset 6\nplt.figure(figsize=(12, 4))\nplt.plot(df6['timestamp'], df6['value'], linewidth=0.8, color='navy')\n\n# Mark break points\nbreak_points_plot = [800, 1500, 2100]\nfor bp in break_points_plot:\n    plt.axvline(x=bp, color='red', linestyle='--', alpha=0.5, linewidth=1)\n\nplt.title('Dataset 6: Non-Stationary with Trend Breaks')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sm5sslgp84s",
   "source": "## Dataset 7: AR(1) with Skewed Noise\n\n**Purpose**: Test interval symmetry assumptions  \n**Model**: y_t = 0.7 * y_{t-1} + ε, where ε ~ Skew-normal  \n**What's varying**: Noise is asymmetric",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lq1gzdtjlgi",
   "source": "# Dataset 7: AR(1) with skewed noise\nnp.random.seed(7)\nn = 2500\nar_coef = 0.7\n\n# Skew-normal parameters\nskewness = 5  # positive skewness\n\n# Generate AR(1) process with skew-normal noise\ny = np.zeros(n)\ny[0] = stats.skewnorm.rvs(skewness)\nfor t in range(1, n):\n    y[t] = ar_coef * y[t-1] + stats.skewnorm.rvs(skewness)\n\n# Create DataFrame\ndf7 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'noise_type': 'skew_normal',\n    'skewness_param': skewness\n})\n\n# Save to CSV\ndf7.to_csv('../data/dataset_7_skewed_noise.csv', index=False)\nprint(f\"Dataset 7 generated: {len(df7)} observations\")\nprint(f\"Mean: {df7['value'].mean():.3f}, Std: {df7['value'].std():.3f}\")\nprint(f\"Skewness: {stats.skew(df7['value']):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "biu1q6wkk1r",
   "source": "# Plot Dataset 7\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(df7['timestamp'], df7['value'], linewidth=0.8, color='teal')\nax1.set_title('Dataset 7: AR(1) with Skewed Noise')\nax1.set_xlabel('Time')\nax1.set_ylabel('Value')\nax1.grid(True, alpha=0.3)\n\nax2.hist(df7['value'], bins=50, color='teal', alpha=0.7, edgecolor='black')\nax2.set_title('Distribution (showing skewness)')\nax2.set_xlabel('Value')\nax2.set_ylabel('Frequency')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rc4hlhzq0q8",
   "source": "## Dataset 8: Seasonal + Noise with Varying Seasonal Amplitude\n\n**Purpose**: Test decomposition-based methods  \n**Model**: y_t = trend + seasonal_t * amplitude_t + AR noise  \n**What's varying**: Seasonality strength changes over time",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bu8wac8vl0t",
   "source": "# Dataset 8: Seasonal + noise with varying seasonal amplitude\nnp.random.seed(8)\nn = 2500\nar_coef = 0.5\n\n# Time variable\nt = np.arange(n)\n\n# Trend component\ntrend = 0.01 * t\n\n# Seasonal component (period = 50)\nseasonal_period = 50\nseasonal = np.sin(2 * np.pi * t / seasonal_period)\n\n# Time-varying seasonal amplitude\namplitude_t = 1 + 2 * np.sin(2 * np.pi * t / 500)\n\n# Generate AR(1) process for noise\nnoise = np.zeros(n)\nnoise[0] = np.random.normal(0, 0.5)\nfor i in range(1, n):\n    noise[i] = ar_coef * noise[i-1] + np.random.normal(0, 0.5)\n\n# Combine components\ny = trend + seasonal * amplitude_t + noise\n\n# Create DataFrame\ndf8 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'trend': trend,\n    'seasonal_amplitude': amplitude_t,\n    'seasonal_component': seasonal * amplitude_t\n})\n\n# Save to CSV\ndf8.to_csv('../data/dataset_8_varying_seasonal.csv', index=False)\nprint(f\"Dataset 8 generated: {len(df8)} observations\")\nprint(f\"Mean: {df8['value'].mean():.3f}, Std: {df8['value'].std():.3f}\")\nprint(f\"Seasonal amplitude range: [{df8['seasonal_amplitude'].min():.3f}, {df8['seasonal_amplitude'].max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zmz7rii38e",
   "source": "# Plot Dataset 8\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n\nax1.plot(df8['timestamp'], df8['value'], linewidth=0.8, color='magenta')\nax1.set_title('Dataset 8: Seasonal with Varying Amplitude')\nax1.set_ylabel('Value')\nax1.grid(True, alpha=0.3)\n\nax2.plot(df8['timestamp'], df8['seasonal_amplitude'], linewidth=0.8, color='red')\nax2.set_ylabel('Seasonal Amplitude')\nax2.set_xlabel('Time')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sk7tucs76ck",
   "source": "## Dataset 9: Mixture of Gaussians Noise\n\n**Purpose**: Test robustness to contamination  \n**Model**: y_t = 0.7 * y_{t-1} + ε, where ε ~ 0.8*N(0,1) + 0.2*N(0,5)  \n**What's varying**: Bimodal distribution with occasional large shocks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yqszxi5gyj9",
   "source": "# Dataset 9: Mixture of Gaussians noise\nnp.random.seed(9)\nn = 2500\nar_coef = 0.7\n\n# Mixture parameters\np_normal = 0.8  # probability of normal component\np_shock = 0.2   # probability of shock component\n\n# Generate mixture noise\ndef mixture_gaussian():\n    if np.random.rand() < p_normal:\n        return np.random.normal(0, 1)\n    else:\n        return np.random.normal(0, 5)\n\n# Generate AR(1) process with mixture noise\ny = np.zeros(n)\nmixture_indicator = np.zeros(n)\ny[0] = mixture_gaussian()\nfor t in range(1, n):\n    epsilon = mixture_gaussian()\n    mixture_indicator[t] = 1 if abs(epsilon) > 2 else 0  # indicator for large shocks\n    y[t] = ar_coef * y[t-1] + epsilon\n\n# Create DataFrame\ndf9 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'shock_indicator': mixture_indicator.astype(int)\n})\n\n# Save to CSV\ndf9.to_csv('../data/dataset_9_mixture_gaussian.csv', index=False)\nprint(f\"Dataset 9 generated: {len(df9)} observations\")\nprint(f\"Mean: {df9['value'].mean():.3f}, Std: {df9['value'].std():.3f}\")\nprint(f\"Large shocks: {df9['shock_indicator'].sum()} ({100*df9['shock_indicator'].mean():.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j1gmk7sah8o",
   "source": "# Plot Dataset 9\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(df9['timestamp'], df9['value'], linewidth=0.8, color='darkred')\nshock_times = df9[df9['shock_indicator'] == 1]['timestamp']\nshock_values = df9[df9['shock_indicator'] == 1]['value']\nax1.scatter(shock_times, shock_values, color='red', s=20, alpha=0.5, label='Large shocks')\nax1.set_title('Dataset 9: AR(1) with Mixture of Gaussians Noise')\nax1.set_xlabel('Time')\nax1.set_ylabel('Value')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\nax2.hist(df9['value'], bins=50, color='darkred', alpha=0.7, edgecolor='black')\nax2.set_title('Distribution (bimodal/contaminated)')\nax2.set_xlabel('Value')\nax2.set_ylabel('Frequency')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i8w0rdxvz2",
   "source": "## Dataset 10: Long-Memory Process (ARFIMA)\n\n**Purpose**: Test methods on slowly decaying correlations  \n**Model**: Fractionally integrated process with d ≈ 0.3  \n**What's varying**: Long-range dependence (autocorrelation structure)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ywiujl7ewm",
   "source": "# Dataset 10: Long-memory process (ARFIMA)\nnp.random.seed(10)\nn = 2500\nd = 0.3  # fractional differencing parameter\n\n# Generate fractional differencing coefficients\ndef fracdiff_coef(d, n_coefs):\n    \"\"\"Generate fractional differencing coefficients\"\"\"\n    coefs = np.zeros(n_coefs)\n    coefs[0] = 1\n    for k in range(1, n_coefs):\n        coefs[k] = coefs[k-1] * (k - 1 - d) / k\n    return coefs\n\n# Get coefficients\nmax_lag = 100\ncoefs = fracdiff_coef(d, max_lag)\n\n# Generate ARFIMA process\n# Start with white noise\nwhite_noise = np.random.normal(0, 1, n + max_lag)\n\n# Apply fractional integration (inverse of fractional differencing)\ny = np.zeros(n)\nfor t in range(n):\n    # Sum over past white noise with fractional weights\n    y[t] = sum(coefs[k] * white_noise[t + max_lag - k] for k in range(min(t + 1, max_lag)))\n\n# Create DataFrame\ndf10 = pd.DataFrame({\n    'timestamp': range(n),\n    'value': y,\n    'fracdiff_param': d\n})\n\n# Save to CSV\ndf10.to_csv('../data/dataset_10_arfima.csv', index=False)\nprint(f\"Dataset 10 generated: {len(df10)} observations\")\nprint(f\"Mean: {df10['value'].mean():.3f}, Std: {df10['value'].std():.3f}\")\nprint(f\"Fractional differencing parameter d: {d}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "x001lqh4tdg",
   "source": "# Plot Dataset 10\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(df10['timestamp'], df10['value'], linewidth=0.8, color='darkgreen')\nax1.set_title('Dataset 10: ARFIMA Process (Long Memory)')\nax1.set_xlabel('Time')\nax1.set_ylabel('Value')\nax1.grid(True, alpha=0.3)\n\n# Compute and plot autocorrelation\nfrom pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df10['value'], ax=ax2, color='darkgreen')\nax2.set_title('Autocorrelation (slowly decaying)')\nax2.set_xlabel('Lag')\nax2.set_xlim([0, 100])\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xrxvk8anukm",
   "source": "## Summary Table\n\nOverview of all generated datasets and what assumptions they test:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "heq7ufhs6zv",
   "source": "# Create summary table\nsummary_data = {\n    'Dataset': [\n        '1. Baseline AR(1)',\n        '2. Heavy-tailed noise',\n        '3. GARCH-like',\n        '4. Time-varying variance',\n        '5. Regime-switching',\n        '6. Trend breaks',\n        '7. Skewed noise',\n        '8. Varying seasonal',\n        '9. Mixture of Gaussians',\n        '10. ARFIMA'\n    ],\n    'What\\'s Varying': [\n        'Nothing (baseline)',\n        'Noise distribution (heavy tails)',\n        'Variance (GARCH)',\n        'Variance (deterministic pattern)',\n        'Regime switches',\n        'Trend breaks',\n        'Skewed noise',\n        'Seasonal amplitude',\n        'Bimodal noise',\n        'Long memory'\n    ],\n    'Traditional PI Weakness Tested': [\n        'None — should work well',\n        'Gaussian assumption',\n        'Constant variance assumption',\n        'Constant variance assumption',\n        'Stationarity assumption',\n        'Stationarity assumption',\n        'Symmetric distribution assumption',\n        'Fixed seasonal pattern assumption',\n        'Unimodal distribution assumption',\n        'Short-range dependence assumption'\n    ],\n    'File': [\n        'dataset_1_baseline_ar1.csv',\n        'dataset_2_heavy_tailed.csv',\n        'dataset_3_garch.csv',\n        'dataset_4_timevarying_variance.csv',\n        'dataset_5_regime_switching.csv',\n        'dataset_6_trend_breaks.csv',\n        'dataset_7_skewed_noise.csv',\n        'dataset_8_varying_seasonal.csv',\n        'dataset_9_mixture_gaussian.csv',\n        'dataset_10_arfima.csv'\n    ]\n}\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"\\n\" + \"=\"*100)\nprint(\"SUMMARY OF SYNTHETIC DATASETS\")\nprint(\"=\"*100)\nprint(summary_df.to_string(index=False))\nprint(\"=\"*100)\nprint(f\"\\nAll datasets saved to ../data/ directory\")\nprint(f\"Total datasets: {len(summary_df)}\")\nprint(f\"Total observations per dataset: ~2500\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dhvvlipm3rv",
   "source": "## Comprehensive Visualization\n\nPlot all 10 datasets in a single grid for comparison:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i9vx7kvbgur",
   "source": "# Comprehensive visualization of all 10 datasets\nfig, axes = plt.subplots(5, 2, figsize=(15, 18))\naxes = axes.flatten()\n\ndatasets = [\n    (df1, '1. Baseline AR(1)', 'blue'),\n    (df2, '2. Heavy-tailed (Student-t)', 'orange'),\n    (df3, '3. GARCH-like', 'green'),\n    (df4, '4. Time-varying Variance', 'purple'),\n    (df5, '5. Regime-switching', 'brown'),\n    (df6, '6. Trend Breaks', 'navy'),\n    (df7, '7. Skewed Noise', 'teal'),\n    (df8, '8. Varying Seasonal', 'magenta'),\n    (df9, '9. Mixture of Gaussians', 'darkred'),\n    (df10, '10. ARFIMA (Long Memory)', 'darkgreen')\n]\n\nfor idx, (df, title, color) in enumerate(datasets):\n    axes[idx].plot(df['timestamp'], df['value'], linewidth=0.6, color=color)\n    axes[idx].set_title(title, fontsize=10, fontweight='bold')\n    axes[idx].set_xlabel('Time', fontsize=8)\n    axes[idx].set_ylabel('Value', fontsize=8)\n    axes[idx].grid(True, alpha=0.3)\n    axes[idx].tick_params(labelsize=8)\n\nplt.suptitle('Synthetic Time Series Datasets for Conformal Prediction Comparison', \n             fontsize=14, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('../data/all_datasets_overview.png', dpi=150, bbox_inches='tight')\nprint(\"Comprehensive plot saved to ../data/all_datasets_overview.png\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}